[
["index.html", "Multivariate Stats 1 Lesson 2 1.1 Random Variables and Vectors 1.2 Variance and Corrected Variance 1.3 Exercises", " Multivariate Stats Tiago dos Santos 2018-11-05 1 Lesson 2 1.1 Random Variables and Vectors 1.1.1 Elements of probability A random variable \\(\\textbf{X}\\) is a function that takes an event space and return a value: \\[ X: \\Omega \\rightarrow {\\rm I\\!R}\\] The behavious of a random variable \\(X\\) can be described by both probability function and distribution function: Distribution Function: \\(F(x) = P(\\text{x} \\leq x) (x \\in {\\rm I\\!R}; 0 \\leq F(x) \\leq 1 ) \\text{F(x) is differentiable}\\) For discrete variables, Probability Mass Function: For continuous variables, Probability Density Function: 1.1.2 1.2 Variance and Corrected Variance 1.2.1 Variance \\[ S_{n}^2 = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(X_{i} - \\overline{X})^2 \\] \\[ S_{n}^2 = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(X_{i} - \\overline{X})(X_{i} - \\overline{X}) \\] 1.2.2 Corrected Variance \\[ S_{n-1}^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n-1}(X_{i} - \\overline{X})^2 \\] \\[ S_{n-1}^2 = \\frac{1}{n}\\sum\\limits_{i=1}^{n-1}(X_{i} - \\overline{X})(X_{i} - \\overline{X}) \\] 1.2.3 Covariance \\[Cov(x,y) = \\frac{1}{n-1}\\sum\\limits_{i=1}^{n}(x_{i} - \\overline{x})(y_{i} - \\overline{y}) \\] \\[Cov(x,y) = Cov(y,x) \\] 1.2.4 Pearson Correlation Coefficient \\[r_{xy} = \\frac{Cov(x,y)}{\\sqrt{S_{x}^2 \\times S_{y}^2}}\\] The domain of this coefficient is [-1, 1] \\[ \\Sigma = V^{\\frac{1}{2}} \\] 1.3 Exercises X = matrix(c(42,52,48,58,4,5,4,3),4) XMeans &lt;- apply(X, 2, mean) XVars &lt;- var(X) Xcor &lt;- cor(X) dados &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;data1.xlsx&quot;))) aplpack::faces(HSAUR3::USairpollution[1:9,], print.info = F) meanVector &lt;- c(5,10) Sigma &lt;- matrix(c(9,16,16,64),2) Sigma.eigen &lt;- eigen(Sigma) The eigen values are 68.3158765, 4.6841235 TO obtain the ellipse containing 95% of the population, we must calculate \\[(x-\\mu)&#39; \\times \\Sigma^{-1} \\times (x-\\mu)&#39; &lt;= \\chi^2_{(2)0.95}\\] "],
["lesson-3.html", "2 Lesson 3", " 2 Lesson 3 dados2 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data2.xlsx&quot;))) s &lt;- seq(min(dados2$imc),max(dados2$imc), length.out = length(dados2$imc)) hist( dados2$imc, probability = T, xlim=c(min(dados2$imc),max(dados2$imc)), main=NULL, xlab=NULL) lines(s,dnorm(s,mean(dados2$imc),sd(dados2$imc))) plot(density(dados2[,1]),ylim=c(0,0.3),main = &quot; &quot;,ylab = &quot;Densidade&quot;) lines(density(dados2[,1],bw = 0.3),col=&quot;red&quot;) lines(density(dados2[,1],bw = 0.1),col=&quot;blue&quot;) legend(&quot;topright&quot;,col=c(1,2,4),lty=1,legend = c(&quot;bw=default&quot;,&quot;bw=0.3&quot;,&quot;bw=0.1&quot;)) plot(density(dados2[,1]),ylim=c(0,0.3),main = &quot; &quot;,ylab = &quot;Densidade&quot;) lines(density(dados2[,1],kernel = &quot;rectangular&quot;),col=&quot;red&quot;) lines(density(dados2[,1],kernel = &quot;epanechnikov&quot;),col=&quot;blue&quot;) legend(&quot;topright&quot;,col=c(1,2,4),lty=1,legend = c(&quot;default&quot;,&quot;rectangular&quot;,&quot;epanechnikov&quot;)) dados1 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data1.xlsx&quot;), col_names = F)) par(mfrow=c(3,2)) plots &lt;- apply(dados1,2,function(iVar){ s &lt;- seq(min(iVar),max(iVar), length.out = length(iVar)) hist( iVar, probability = T, xlim=c(min(iVar),max(iVar)), main=NULL, xlab=NULL) lines(s,dnorm(s,mean(iVar),sd(iVar)),col=&quot;red&quot;) lines(density(iVar,kernel = &quot;rectangular&quot;),col=&quot;blue&quot;) legend(&quot;topright&quot;,col=c(1,2,4),lty=1,legend = c(&quot;histogram&quot;,&quot;dnorm&quot;,&quot;gaussian&quot;)) }) par(mfrow=c(1,1)) 2.0.1 Estat??sticas Ordinais 2.0.1.1 Minimum \\[ x_\\left(1\\right) = x_\\left(1n\\right) \\] 2.0.1.2 Maximum \\[ x_\\left(n\\right) = x_\\left(nn\\right) \\] 2.0.1.3 Inverso da fun????o distribui????o Gaussiana qnorm(0.975) ## [1] 1.959964 2.0.1.4 QQ Plot x &lt;- c(-1.00,-0.10,0.16,0.41,0.62,0.80,1.26,1.54,1.71,2.3) xdf &lt;- as.data.frame(x) names(xdf) &lt;- c(&quot;values&quot;) nLength &lt;- length(xdf$values) xdf$empProb &lt;- sapply(1:nLength,function(idx){ (idx-0.5)/nLength }) xdf$theoreticProb &lt;- sapply(xdf$empProb,function(quantile){ qnorm(quantile) }) knitr::kable(xdf) values empProb theoreticProb -1.00 0.05 -1.6448536 -0.10 0.15 -1.0364334 0.16 0.25 -0.6744898 0.41 0.35 -0.3853205 0.62 0.45 -0.1256613 0.80 0.55 0.1256613 1.26 0.65 0.3853205 1.54 0.75 0.6744898 1.71 0.85 1.0364334 2.30 0.95 1.6448536 plot(xdf$empProb , xdf$theoreticProb ) car::qqp(x,distribution=&quot;norm&quot;) ## [1] 1 10 defaultMar &lt;- c(5, 4, 4, 2) + 0.1 par(mfrow=c(3,2),mar=c(0,0,0,0)) plots &lt;- apply(dados1,2,function(iVar){ car::qqp(iVar,distribution=&quot;norm&quot;) }) par(mfrow=c(1,1),defaultMar) 2.0.2 Transforming to Distance (slide 27) x1 &lt;- c(126974,96933,86656,63438,55264,50976,39069,36156,35209,32416) x2 &lt;- c(4224,3835,3510,3758,3939,1809,2946, 359,2480,2413) X &lt;- matrix(c(x1,x2),10,2) mx &lt;- colMeans(X) S &lt;- var(x) d2 &lt;- c() distanceCsquare &lt;- function(){ el - mean } for(i in 1:nrow(X)){ d2[i] &lt;- (t(X[i,]) - colMeans(X))%*%solve(var(X))%*%(X[i,]-colMeans(X)) } ## o u ?? que tem distribui????o beta ## quantis teoriocos qui-quadrado qi_chi &lt;- round(qchisq(xdf$empProb, ncol(X)),3) qi_chi ## [1] 0.103 0.325 0.575 0.862 1.196 1.597 2.100 2.773 3.794 5.991 xdf$theoreticProbQuiSq &lt;- qi_chi ## quantis teoricos de beta qi_beta &lt;- round(qbeta(xdf$empProb, shape1 = ncol(X)/2, shape2=(nrow(X)-ncol(X)-1)/2),3) qi_beta ## [1] 0.015 0.045 0.079 0.116 0.157 0.204 0.259 0.327 0.418 0.575 xdf$theoreticProbQuiSq &lt;- qi_beta ## aqui deveria ser u em vez de d2 par(mfrow=c(1,2)) plot(xdf$theoreticProbQuiSq, d2) car::qqp(d2,distribution=&quot;chisq&quot;,df=ncol(X)) ## [1] 8 1 hypothesisVal &lt;- c(11,3) X &lt;- matrix(c(6,10,8,9,6,3),3) Xmeans &lt;- colMeans(X) S &lt;- var(X) Sminus1 &lt;- solve(S) t(nrow(X)*(Xmeans - hypothesisVal))%*%Sminus1%*%(Xmeans - hypothesisVal) ## [,1] ## [1,] 7 "],
["lesson-4.html", "3 Lesson 4 3.1 Slide 9 exercice 3.2 Slide 10 exercise", " 3 Lesson 4 3.1 Slide 9 exercice X = matrix(c(6,10,8,9,6,3),3) X.mean &lt;- colMeans(X) X.S &lt;- var(X) #covariance matrix X.Sinv &lt;- solve(X.S) TSquare_Hotelling &lt;- function(hypothesis,n,vectorMean,covInverse){ n*t((vectorMean - hypothesis))%*%covInverse%*%(vectorMean - hypothesis) } hypothesis &lt;- c(11,3) TSquare_Hotelling(hypothesis, nrow(X), X.mean, X.Sinv) ## [,1] ## [1,] 7 We do not know the quantiles of the \\(T^2 Hotelling\\), but we know that the function \\(F\\) emulates the \\(T^2 Hotelling\\) and therefore we can do alpha = 0.05 4*qf(1-alpha,2,1) ## [1] 798 3.2 Slide 10 exercise dados3 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;data3.xlsx&quot;))) # trnasformar dados x1 &lt;- dados3[,1]^(1/4) x2 &lt;- dados3[,2]^(1/4) hypothesis &lt;- c(0.562,0.589) x1&lt;-dados3[,1]^(1/4) x2&lt;-dados3[,2]^(1/4) x&lt;-matrix(c(x1,x2),42,2) m&lt;-colMeans(x) s&lt;-var(x) hip&lt;-matrix(c(0.562,0.589),2,1) #n&lt;-dim(dados3)[1] #T hotelling T_qua=nrow(x)*t(m-hip)%*%solve(s)%*%(m-hip) T_qua ## [,1] ## [1,] 1.2573 vc=((ncol(x)*(nrow(x)-1))/((nrow(x)-ncol(x))))*qf(df1 = ncol(x),df2 = nrow(x)-ncol(x),p = 0.95) #calculo do valor observado round(vc,3) ## [1] 6.625 eigenS &lt;- eigen(s) eval &lt;- eigenS$values evec &lt;- eigenS$vectors #vc is valor cr??tico m - sqrt(eval[1])*sqrt(vc)*evec[,1] ## [1] 0.2710919 0.3073648 "],
["lesson-5.html", "4 Lesson 5 4.1 ANOVA 4.2 Adding the Samples column 4.3 Exercicio 2 4.4 Manova 4.5 avaliar a Homocedasticidade das variáveis", " 4 Lesson 5 4.1 ANOVA X &lt;- t(matrix(c(9,6,9,0,2,3,1,2,3,2,7,4,0,8,9,7),2,byrow=T)) S &lt;- var(X) TotalMatrix &lt;- (dim(X)[1]-1)*S #samples extrated from population 1 n1 &lt;- 3 S1 &lt;- var(X[1:3,]) #samples extrated from population 2 n2 &lt;- 2 S2 &lt;- var(X[4:5,]) #samples extrated from population 3 n3 &lt;- 3 S3 &lt;- var(X[6:8,]) # Commum Covariance Matrix that we need to apply ANOVA W = (n1-1)*S1 + (n2-1)*S2 + (n3-1)*S3 B &lt;- TotalMatrix - W # observed test statistics tObs &lt;- det(W)/det(TotalMatrix) #alternativamente, posso calcular eigenValues &lt;- eigen(B%*%solve(W))$values tObs.alternative &lt;- prod(1/(1+eigenValues)) # Transform the last value (that has an unkown distribution) to an F distribution through the following transform: (aka F approximation) tObs.F &lt;- (((dim(X)[1]-dim(X)[2]-2)/dim(X)[2])) * ((1-sqrt(tObs.alternative))/(sqrt(tObs.alternative))) g = 3 (groups number) p = 2 (dimensions number) $$ F_{2p, 2(n-p-2)} $$ significanceLevel &lt;- 0.01 criticRegion &lt;- qf(1-significanceLevel, 2*dim(X)[2], 2*(dim(X)[1]-dim(X)[2]-2)) p_value &lt;- (1-pf(tObs.F,4,8)) Ambas são verdadeiras (uma implica a outra) - Rejeita-se porque p-value &lt;= 0.01 - Rejeita-se porque tObs transformado em F é &gt;= criticRegion Agora nós rejeitamos mas não sabemos porquê: - foi porque todas as dimensões são diferentes? (dimensões ou grupos?) - ou são porque uma é diferente? Para tal, vamos analizar os intervalos de confiança 4.2 Adding the Samples column grupo &lt;- c(rep(1,3),rep(2,2),rep(3,3)) Xg &lt;- cbind(grupo, X) 4.3 Exercicio 2 Intervalos de Confiança a 99% m &lt;- aggregate(Xg[,-1],by=list(G = Xg[,1]), FUN=mean) g &lt;- length(unique(Xg[,1])) w1 &lt;- W[1,1] w2 &lt;- W[2,2] #numero de comparações #m &lt;- 2*(g*(g-1))/2 alpha &lt;- 0.01 t&lt;-qt(alpha,p = 1-(alpha* dim(X)[1])/2, df = dim(X)[1]-g) colnames(m) &lt;- c(&quot;G&quot;,&quot;x1&quot;,&quot;x2&quot;) varsComp &lt;- c(&quot;x1&quot;,&quot;x2&quot;) groups &lt;- list(c(1,2),c(2,3),c(1,3)) comparisonList &lt;- list() for(currVar in varsComp){ for(group in groups){ res &lt;- (m[group[1],currVar]-m[group[2],currVar])-t*sqrt( w1/(dim(X)[1]-g)*(1/n1 + 1/n2) ) comparisonList[length(comparisonList)+1] = list(var = currVar, cmp = group, res=res) } } ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length ## Warning in comparisonList[length(comparisonList) + 1] &lt;- list(var = ## currVar, : number of items to replace is not a multiple of replacement ## length (m[1,&quot;x1&quot;]-m[2,&quot;x1&quot;])-t*sqrt( w1/(dim(X)[1]-g)*(1/n1 + 1/n2) ) ## [1] 4.152474 4.4 Manova dados5 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data5.xlsx&quot;))) dados5$fator1 &lt;- as.factor(dados5$fator1) dados5$fator2 &lt;- as.factor(dados5$fator2) #m &lt;- manova(cbind(x1,x2,x3)*fator1*fator2, data=dados5) #summary(m, test = &quot;Wilks&quot;) #m1 &lt;- aov(x1*as.factor(fator1)*as.factor(fator2),data=dados5) #summary(m1) #m2 &lt;- aov(x2*as.factor(fator1)*as.factor(fator2),data=dados5) #summary(m2) #m3 &lt;- aov(x3*as.factor(fator1)*as.factor(fator2),data=dados5) #summary(m3) 4.5 avaliar a Homocedasticidade das variáveis dados7 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data7.xlsx&quot;), col_names = F)) colnames(dados7) &lt;- c(&quot;G&quot;,&quot;X1&quot;,&quot;X2&quot;,&quot;X3&quot;,&quot;X4&quot;) S1 &lt;- var(dados7[dados7$G == 1,-1]) S2 &lt;- var(dados7[dados7$G == 2,-1]) n1 &lt;- sum(dados7$G==1) n2 &lt;- sum(dados7$G==2) #k is the number of groups Spooled &lt;- ((n1-1)*S1+(n2-1)*S2)/(n1+n2-2) num &lt;- det(S1)^((n1-1)/2) * det(S2)^((n2-1)/2) den &lt;- det(Spooled)^((n1+n2-2)/2) lambda &lt;- num/den p &lt;- dim(dados7[,-1])[2] g &lt;- length(unique(dados7$G)) ni &lt;- c(n1,n2) c1 &lt;- (sum(1/(ni-1)) - 1/(sum(ni-1)))*((2*p^2 + 3*p - 1)/(6*(p+1)*(g-1))) Ustar &lt;- -2*(1-c1)*log(lambda) We know that the transformation \\(-2(1 - c1)ln\\Lambda\\) follows a \\(\\chi^2_{(g-1)(p+1)}\\) qchisq(0.95, g-1, p+1) ## [1] 15.06155 Therefore we do not reject H0. All automated now! #biotools::boxM(data= dados7[,-1], grouping = dados7$G) "],
["lesson-6.html", "5 Lesson 6 5.1 avaliar a Homocedasticidade das variáveis 5.2 Contrast Matrix 5.3 Discriminant Analysis", " 5 Lesson 6 5.1 avaliar a Homocedasticidade das variáveis dados7 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data7.xlsx&quot;), col_names = F)) colnames(dados7) &lt;- c(&quot;G&quot;,&quot;X1&quot;,&quot;X2&quot;,&quot;X3&quot;,&quot;X4&quot;) S1 &lt;- var(dados7[dados7$G == 1,-1]) S2 &lt;- var(dados7[dados7$G == 2,-1]) n1 &lt;- sum(dados7$G==1) n2 &lt;- sum(dados7$G==2) #k is the number of groups Spooled &lt;- ((n1-1)*S1+(n2-1)*S2)/(n1+n2-2) num &lt;- det(S1)^((n1-1)/2) * det(S2)^((n2-1)/2) den &lt;- det(Spooled)^((n1+n2-2)/2) lambda &lt;- num/den p &lt;- dim(dados7[,-1])[2] g &lt;- length(unique(dados7$G)) ni &lt;- c(n1,n2) c1 &lt;- (sum(1/(ni-1)) - 1/(sum(ni-1)))*((2*p^2 + 3*p - 1)/(6*(p+1)*(g-1))) Ustar &lt;- -2*(1-c1)*log(lambda) We know that the transformation \\(-2(1 - c1)ln\\Lambda\\) follows a \\(\\chi^2_{(g-1)(p+1)}\\) qchisq(0.95, g-1, p+1) ## [1] 15.06155 Therefore we do not reject H0. All automated now! #biotools::boxM(data= dados7[,-1], grouping = dados7$G) 5.2 Contrast Matrix data &lt;- matrix(c(30,21,21,14,22,13,22,5,20,13,18,17,12,7,16,14,23,24,23,8),5,byrow = T) n &lt;- dim(data)[1] q &lt;- dim(data)[2] C &lt;- matrix(c(1,1,-1,-1,1,-1,1,-1,1,-1,-1,1),3,byrow=T) S &lt;- var(data) xbar &lt;- colMeans(data) Tsqrt &lt;- n*t(C%*%xbar)%*%solve(C%*%S%*%t(C))%*%(C%*%xbar) ## Now transforming from T^2 to F by FtoTsqrt &lt;- ((n-1)*(q-1))/(n-q+1) criticRegion &lt;- FtoTsqrt*qf(0.95,q-1,n-q+1) library(plotly) ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 3.4.4 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout dados7 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data7.xlsx&quot;), col_names = F)) colnames(dados7) &lt;- c(&quot;G&quot;,&quot;X1&quot;,&quot;X2&quot;,&quot;X3&quot;,&quot;X4&quot;) alpha &lt;- 0.01 significanceLevel &lt;- 1 - alpha G1 &lt;- dados7[dados7$G == 1,-1] G2 &lt;- dados7[dados7$G == 2,-1] G1.colmeans &lt;- colMeans(G1) G1.n &lt;- dim(G1)[1] G2.colmeans &lt;- colMeans(G2) G2.n &lt;- dim(G2)[1] plotly::plot_ly( data=G1, y=~G1.colmeans, x=~1:length(G1.colmeans), type=&quot;scatter&quot;, mode=&quot;lines&quot;, name=&quot;group1&quot; ) %&gt;% plotly::add_lines( data=G2, y=~G2.colmeans, x=~1:length(G2.colmeans), name=&quot;group2&quot; ) %&gt;% plotly::layout( yaxis=list(title=&quot;Statistic&quot;), xaxis=list(title=&quot;Psy-Indicators&quot;) ) ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 G1.S &lt;- var(dados7[dados7$G == 1,-1]) G2.S &lt;- var(dados7[dados7$G == 2,-1]) C &lt;- matrix(c(-1,1,0,0,0,-1,1,0,0,0,-1,1),3,byrow=T) Spooled &lt;- ((G1.n - 1)*G1.S+(G2.n - 1)*G2.S)/((G1.n-1)+(G2.n-1)) f1 &lt;- t(C%*%(G1.colmeans-G2.colmeans)) f2 &lt;- solve((1/G1.n + 1/G2.n)*C%*%Spooled%*%t(C)) f3 &lt;- t(f1) Tsqrt &lt;- f1%*%f2%*%f3 #T^2_{q-1}(n-2) n &lt;- dim(dados7)[1] q &lt;- dim(dados7)[2] FtoTsqrt &lt;- ((n-1)*(q-1))/(n-q+1) criticRegion &lt;- FtoTsqrt*qf(significanceLevel,q-1,n-q+1) p_value &lt;- 1-(pf(Tsqrt/FtoTsqrt,3,n-q+1)) 5.3 Discriminant Analysis Assumption: The groups (with mean vectors \\(\\mu_1\\) and \\(\\mu_2\\)) have the same covariance matrix data &lt;- as.data.frame(matrix(c(33,60,36,61,35,64,38,63,40,65,35,57,36,59,38,59,39,61,41,63,43,65,41,59),12,byrow=T)) n1 &lt;- 5 n2 &lt;- 7 data$G &lt;- c(rep(1,n1),rep(2,n2)) xbar_g1 &lt;- colMeans(data[data$G == 1,c(1,2)]) xbar_g2 &lt;- colMeans(data[data$G == 2,c(1,2)]) S1 &lt;- var(data[data$G == 1,c(1,2)]) S2 &lt;- var(data[data$G == 2,c(1,2)]) Spooled &lt;- ((n1-1)*S1 + (n2-1)*S2)/((n1-1)+(n2-1)) a &lt;- solve(Spooled)%*%(xbar_g1 - xbar_g2) c1 &lt;- c(-2,0,-1,0,2,1,1,0,-1) c2 &lt;- c(5,3,1,6,4,2,-2,0,-4) groups &lt;- c(1,2,3) G &lt;- rep(groups,each=3) X &lt;- data.frame(cbind(G,c1,c2)) n &lt;- dim(X)[1] S &lt;- var(X[,-1]) Slist &lt;- lapply(groups, function(group){ var(X[X$G==group,-1]) }) nlist &lt;- lapply(groups, function(group){ dim(X[X$G==group,-1])[1] }) T &lt;- (n-1)*S W &lt;- matrix(c(0,0,0,0),2) for(i in 1:length(nlist)){ W = W + (nlist[[i]]-1)*Slist[[i]] } B &lt;- T - W #UNFINISHED! AND MAYBE WRONG library(&quot;MASS&quot;) ## Warning: package &#39;MASS&#39; was built under R version 3.4.4 ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select #wine&lt;-read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;,sep=&quot;,&quot;) #wine.lda &lt;- lda(wine$V1 ~ wine$V2 + wine$V3 + wine$V4 + wine$V5 + wine$V6 + wine$V7 + wine$V8 + wine$V9 + wine$V10 + wine$V11 + wine$V12 + wine$V13 +wine$V14) #wine.lda$scaling #wine.lda.values &lt;- predict(wine.lda, wine[2:14]) #ldahist(data = wine.lda.values$x[,1], g=wine$V1) #ldahist(data = wine.lda.values$x[,2], g=wine$V1) #plot(wine.lda.values$x[,1],wine.lda.values$x[,2],xlab = &quot;LD1&quot;,ylab = &quot;LD2&quot;, col=wine$V1,pch=16) "],
["lesson-7.html", "6 Lesson 7 6.1 Análise de Discriminantes 6.2 PCA", " 6 Lesson 7 6.1 Análise de Discriminantes c1 &lt;- c(-2,0,-1,0,2,1,1,0,-1) c2 &lt;- c(5,3,1,6,4,2,-2,0,-4) groups &lt;- c(1,2,3) G &lt;- rep(groups,each=3) X &lt;- data.frame(cbind(G,c1,c2)) n &lt;- dim(X)[1] p &lt;- dim(X)[2]-1 g &lt;- length(groups) alpha &lt;- 0.05 S &lt;- var(X[,-1]) Slist &lt;- lapply(groups, function(group){ var(X[X$G==group,-1]) }) nlist &lt;- lapply(groups, function(group){ dim(X[X$G==group,-1])[1] }) T &lt;- (n-1)*S W &lt;- matrix(rep(0,p*p),2) for(i in 1:length(nlist)){ W = W + (nlist[[i]]-1)*Slist[[i]] } Spooled &lt;- W/(sum(unlist(nlist))-length(nlist)) B &lt;- T - W eigenDisc &lt;- eigen(solve(W)%*%B) Now we need to understand if the second eigen value is significantly different from 0, and that depends on the variability of the distribution. For this, we will use the Wilk’s lambda test stats: \\[ - \\left(n-1-\\frac{p+g}{2}\\right) ln \\Lambda^* \\approx \\chi^2_{p(g-1)} \\] And now we want to test \\(H_0 : \\lambda_i = 0(i = 1,...,s)\\), where \\(s\\) is the number of non-null eigen values. We need to test if each eigen value is significantly different than 0. Therefore, for the m-esime test we have: \\[ \\Lambda^{*}_{m} = \\Pi^{s}_{i=m} \\frac{1}{1+l_i} \\] nEigen &lt;- length(eigenDisc$values) Lw &lt;- sapply(1:nEigen,function(idx){ prod(1/(1+eigenDisc$values[idx:nEigen])) }) #Critic Values CVs &lt;- sapply(1:nEigen,function(idx){ qchisq(0.95,(p-idx+1)*(g-idx)) }) #\\left(n-1-\\frac{p+g}{2}\\right) ln \\Lambda^* \\approx \\chi^2_{p(g-1)} LwChisqr &lt;- sapply(Lw,function(currLw){ -(n-1-((p+g)/2))*log(currLw) }) Lw &gt; CVs ## [1] FALSE FALSE Do we reject any test (for critic regions)? First eigen (0.1357905) VS critic region (9.487729): FALSE Second eigen (0.5251115) VS critic region (3.8414588): FALSE pValues &lt;- sapply(1:nEigen,function(idx){ 1-pchisq(Lw[idx],(p-idx+1)*(g-idx)) }) pValues &lt; alpha ## [1] FALSE FALSE Do we reject any test (for p values)? First eigen (0.9977968) VS critic region (0.05): FALSE Second eigen (0.4686694) VS critic region (0.05): FALSE m &lt;- aggregate(X[,2:3],list(X[,1]),mean) classifications &lt;- apply(X,1,function(obs){ print(obs) obs &lt;- unlist(c(unname(obs[c(2,3)]))) centered &lt;- obs - as.matrix(m[,-1]) dists &lt;- diag(centered%*%solve(Spooled)%*%t(centered)) return(which.min(dists)) }) ## G c1 c2 ## 1 -2 5 ## G c1 c2 ## 1 0 3 ## G c1 c2 ## 1 -1 1 ## G c1 c2 ## 2 0 6 ## G c1 c2 ## 2 2 4 ## G c1 c2 ## 2 1 2 ## G c1 c2 ## 3 1 -2 ## G c1 c2 ## 3 0 0 ## G c1 c2 ## 3 -1 -4 6.2 PCA dados8 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data8.xlsx&quot;))) dados8.scaled &lt;- scale(dados8, center=TRUE,scale=TRUE) S &lt;- var(dados8.scaled) round(S,3) ## WDIM CIRCUM FBEYE EYEHD EARHD JAW ## WDIM 1.000 0.608 0.361 0.060 0.252 0.605 ## CIRCUM 0.608 1.000 0.733 0.336 0.090 0.410 ## FBEYE 0.361 0.733 1.000 0.014 -0.028 0.311 ## EYEHD 0.060 0.336 0.014 1.000 0.297 -0.079 ## EARHD 0.252 0.090 -0.028 0.297 1.000 -0.090 ## JAW 0.605 0.410 0.311 -0.079 -0.090 1.000 dados8.eigen &lt;- eigen(S) eigenAnalysis = data.frame( CP=1:length(dados8.eigen$values), EV= round(dados8.eigen$values,3), Prop_var = round(dados8.eigen$values / length(dados8.eigen$values),3), Prop_cum = round(cumsum(dados8.eigen$values / length(dados8.eigen$values)),3) ) knitr::kable(eigenAnalysis) CP EV Prop_var Prop_cum 1 2.568 0.428 0.428 2 1.369 0.228 0.656 3 0.934 0.156 0.812 4 0.678 0.113 0.925 5 0.321 0.053 0.978 6 0.131 0.022 1.000 plotly::subplot( plotly::plot_ly( eigenAnalysis, x=~CP, y=~EV, type=&quot;scatter&quot;, mode=&quot;lines+markers&quot;, name=&quot;Scree Plot&quot; ), plotly::plot_ly( eigenAnalysis, x=~CP, y=~Prop_cum, type=&quot;scatter&quot;, mode=&quot;lines+markers&quot;, name=&quot;Cum Importance&quot; ) ) "],
["lesson-8.html", "7 Lesson 8 7.1 Interpreting Principal Components 7.2 Exercise", " 7 Lesson 8 dados8 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir, &quot;data8.xlsx&quot;))) footpca&lt;-prcomp(x = dados8,scale. = TRUE) summary(footpca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 1.602 1.1699 0.9663 0.8234 0.56646 0.36191 ## Proportion of Variance 0.428 0.2281 0.1556 0.1130 0.05348 0.02183 ## Cumulative Proportion 0.428 0.6561 0.8117 0.9247 0.97817 1.00000 attributes(footpca) ## $names ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; ## ## $class ## [1] &quot;prcomp&quot; round(footpca$rotation,3) ## PC1 PC2 PC3 PC4 PC5 PC6 ## WDIM -0.510 0.007 -0.447 0.033 0.625 -0.386 ## CIRCUM -0.561 -0.086 0.321 -0.022 0.221 0.725 ## FBEYE -0.463 0.146 0.475 -0.471 -0.307 -0.472 ## EYEHD -0.143 -0.664 0.313 0.594 -0.091 -0.282 ## EARHD -0.109 -0.645 -0.468 -0.489 -0.313 0.127 ## JAW -0.422 0.338 -0.393 0.431 -0.601 0.083 screeplot(footpca,type = &quot;l&quot;,main=&quot;Scree plot&quot;) cbinded_data_pcs &lt;- crosstalk::SharedData$new(cbind.data.frame(as.data.frame(dados8.scaled), as.data.frame(footpca$x))) crosstalk::bscols( d3scatter::d3scatter(cbinded_data_pcs, ~WDIM, ~CIRCUM, color = &quot;blue&quot;,width=&quot;100%&quot;, height=300), d3scatter::d3scatter(cbinded_data_pcs, ~PC1, ~PC2, color = &quot;green&quot;,width=&quot;100%&quot;, height=300) ) dados8.scaled.df &lt;- as.data.frame(dados8.scaled) S &lt;- cov(dados8.scaled.df) S&gt;=0.5 # we have (p^2 - p)/2 distinct correlations, and just 3 of them are &gt;= 0.5 ## WDIM CIRCUM FBEYE EYEHD EARHD JAW ## WDIM TRUE TRUE FALSE FALSE FALSE TRUE ## CIRCUM TRUE TRUE TRUE FALSE FALSE FALSE ## FBEYE FALSE TRUE TRUE FALSE FALSE FALSE ## EYEHD FALSE FALSE FALSE TRUE FALSE FALSE ## EARHD FALSE FALSE FALSE FALSE TRUE FALSE ## JAW TRUE FALSE FALSE FALSE FALSE TRUE pcor &lt;- corpcor::cor2pcor(S) # this is the parcial correlation between variables excluding the influence of all the others # the values must be interpreted as the percentage of variability left explained by removing the influence of all the other variables 7.0.1 Aplying the Maunchly Test / Sphericity Test We assume that the vector is normal multivariate the test stats is \\[ U^* = -\\left(n-1-\\frac{2p^2 + p + 2}{6p}\\right)ln(U) \\] \\[ U = \\Lambda^{2/n} \\] \\[ \\Lambda = \\frac{|S|^{n/2}}{(tr(S)/p)^{np/2}} \\] And therefore: \\[ U = \\frac{p^p|S|}{(tr(S))^p} \\] S.det &lt;- det(S) S.tr &lt;- psych::tr(S) p &lt;- dim(S)[2] n &lt;- dim(dados8)[1] U = ((p^p)*S.det) / (S.tr^p) Lambda &lt;- (S.det^(n/2)) / ((S.tr/p)^(n*p/2)) Lambda^(2/n) ## [1] 0.09350695 Ustar &lt;- -(n - 1 - (2*p^2 + p + 2)/(6*p))*log(U) ## Chi sqrt degrees of freedom chisqrt.degreesfreedom &lt;- (p*(p+1))/2-1 What’s the probability of U* to be greather than 134.547? P(U* &gt; 134.547) 1 - pchisq(Ustar, chisqrt.degreesfreedom) ## [1] 0 And therefore we reject H0. This means that there is at least one correlation that is significantly differente than 0. 7.0.2 Now automated with R code mauchly.test(lm(dados8.scaled~1)) ## ## Mauchly&#39;s test of sphericity ## ## data: SSD matrix from lm(formula = dados8.scaled ~ 1) ## W = 0.093507, p-value &lt; 2.2e-16 7.1 Interpreting Principal Components correlation &lt;- footpca$sdev * footpca$rotation cor.sq &lt;- correlation^2 round(cor.sq,2) ## PC1 PC2 PC3 PC4 PC5 PC6 ## WDIM 0.67 0.00 0.51 0.00 1.00 0.38 ## CIRCUM 0.43 0.01 0.14 0.00 0.07 0.72 ## FBEYE 0.20 0.02 0.21 0.21 0.09 0.21 ## EYEHD 0.01 0.30 0.07 0.24 0.01 0.05 ## EARHD 0.00 0.13 0.07 0.08 0.03 0.01 ## JAW 0.02 0.01 0.02 0.02 0.05 0.00 round((footpca$sdev^2)/p,2) ## [1] 0.43 0.23 0.16 0.11 0.05 0.02 biplot(footpca,cex=0.6) 7.2 Exercise head(MASS::crabs,6) ## sp sex index FL RW CL CW BD ## 1 B M 1 8.1 6.7 16.1 19.0 7.0 ## 2 B M 2 8.8 7.7 18.1 20.8 7.4 ## 3 B M 3 9.2 7.8 19.0 22.4 7.7 ## 4 B M 4 9.6 7.9 20.1 23.1 8.2 ## 5 B M 5 9.8 8.0 20.3 23.0 8.2 ## 6 B M 6 10.8 9.0 23.0 26.5 9.8 summary(MASS::crabs) ## sp sex index FL RW ## B:100 F:100 Min. : 1.0 Min. : 7.20 Min. : 6.50 ## O:100 M:100 1st Qu.:13.0 1st Qu.:12.90 1st Qu.:11.00 ## Median :25.5 Median :15.55 Median :12.80 ## Mean :25.5 Mean :15.58 Mean :12.74 ## 3rd Qu.:38.0 3rd Qu.:18.05 3rd Qu.:14.30 ## Max. :50.0 Max. :23.10 Max. :20.20 ## CL CW BD ## Min. :14.70 Min. :17.10 Min. : 6.10 ## 1st Qu.:27.27 1st Qu.:31.50 1st Qu.:11.40 ## Median :32.10 Median :36.80 Median :13.90 ## Mean :32.11 Mean :36.41 Mean :14.03 ## 3rd Qu.:37.23 3rd Qu.:42.00 3rd Qu.:16.60 ## Max. :47.60 Max. :54.60 Max. :21.60 crabs.prcomp &lt;- prcomp(MASS::crabs[,-seq(1,3)], scale. = TRUE) crabs.prcomp.variance &lt;- crabs.prcomp$sdev^2 crabs.eigenValues.cumsum &lt;- cumsum(crabs.prcomp.variance) crabs.eigen.df = data.frame( CP = 1:length(crabs.eigenValues.cumsum), EV= round(crabs.prcomp.variance,3), Prop_cum = crabs.eigenValues.cumsum/sum(crabs.prcomp.variance) ) 7.2.1 Which PC do I choose? What is the percentage of variability explained by each PC? plotly::subplot( plotly::plot_ly( crabs.eigen.df, x=~CP, y=~EV, type=&quot;scatter&quot;, mode=&quot;lines+markers&quot;, name=&quot;Scree Plot&quot; ), plotly::plot_ly( crabs.eigen.df, x=~CP, y=~Prop_cum, type=&quot;scatter&quot;, mode=&quot;lines+markers&quot;, name=&quot;Cum Importance&quot; ) ) 7.2.1.1 Criterias to choose the PCs Maintain PCs up until 80% variability explained 7.2.2 How to interpret the Principal Component? corr_between_pc_vars &lt;- (crabs.prcomp$rotation*crabs.prcomp$sdev) round(corr_between_pc_vars[,1]^2,3) ## FL RW CL CW BD ## 0.979 0.028 0.010 0.002 0.000 7.2.3 Suitability analysis crabs.scaled &lt;- as.data.frame(scale(MASS::crabs[,-seq(1,3)], center=TRUE,scale=TRUE)) S &lt;- cov(crabs.scaled) S&gt;=0.5 # we have (p^2 - p)/2 distinct correlations, and just 3 of them are &gt;= 0.5 ## FL RW CL CW BD ## FL TRUE TRUE TRUE TRUE TRUE ## RW TRUE TRUE TRUE TRUE TRUE ## CL TRUE TRUE TRUE TRUE TRUE ## CW TRUE TRUE TRUE TRUE TRUE ## BD TRUE TRUE TRUE TRUE TRUE carb.partial.cor &lt;- corpcor::cor2pcor(S) carb.partial.cor ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.0000000 0.44626384 0.3359484 -0.2903388 0.47432971 ## [2,] 0.4462638 1.00000000 -0.4444670 0.5002060 0.08379555 ## [3,] 0.3359484 -0.44446701 1.0000000 0.9542583 0.52966607 ## [4,] -0.2903388 0.50020598 0.9542583 1.0000000 -0.40540239 ## [5,] 0.4743297 0.08379555 0.5296661 -0.4054024 1.00000000 # the values must be interpreted as the percentage of variability left explained by removing the influence of all the other variables The values must be interpreted as the percentage of variability left explained by removing the influence of all the other variables 7.2.3.1 Sphericity Test Our \\(H_0\\) is that our correlation matrix is equal to the Identity matrix. S.det &lt;- det(S) S.tr &lt;- psych::tr(S) p &lt;- dim(S)[2] n &lt;- nrow(MASS::crabs) U = ((p^p)*S.det) / (S.tr^p) Lambda &lt;- (S.det^(n/2)) / ((S.tr/p)^(n*p/2)) Lambda^(2/n) ## [1] 0 Ustar &lt;- -(n - 1 - (2*p^2 + p + 2)/(6*p))*log(U) ## Chi sqrt degrees of freedom chisqrt.degreesfreedom &lt;- (p*(p+1))/2-1 1 - pchisq(Ustar, chisqrt.degreesfreedom) ## [1] 0 If done my the R function mauchly.test(lm(as.matrix(crabs.scaled)~1)) ## ## Mauchly&#39;s test of sphericity ## ## data: SSD matrix from lm(formula = as.matrix(crabs.scaled) ~ 1) ## W = 6.4564e-07, p-value &lt; 2.2e-16 By rejecting \\(H_0\\) we are saying that the are correlations that are significatively different from 0. 7.2.4 Calculate scores crabs.scores &lt;- as.matrix(MASS::crabs[,-seq(1,3)]) %*% crabs.prcomp$rotation "],
["studies-experiments.html", "8 Studies &amp; Experiments 8.1 Matrixes’ Span and Determinantes 8.2 EigenValues and EigenVectors 8.3 Spectral Decomposition", " 8 Studies &amp; Experiments Unfortunately, no one can be told what the Matrix is. You have to see it for yourself - Morpheys 8.1 Matrixes’ Span and Determinantes matrixA2d &lt;- matrix(c(1,0,0,1),2) matrixA2d ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 drawMatrixWithDet(matrixA2d,dim(matrixA2d)[1]) matrixB2d &lt;- matrix(c(1,3,0,1),2) matrixB2d ## [,1] [,2] ## [1,] 1 0 ## [2,] 3 1 drawMatrixWithDet(matrixB2d,dim(matrixB2d)[1]) matrixA &lt;- matrix(c(1,0,0,0,1,0,0,0,1),3) matrixA ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 drawMatrixWithDet(matrixA,dim(matrixA)[1]) ## Warning: &#39;mesh3d&#39; objects don&#39;t have these attributes: &#39;mode&#39;, &#39;line&#39; ## Valid attributes include: ## &#39;type&#39;, &#39;visible&#39;, &#39;showlegend&#39;, &#39;legendgroup&#39;, &#39;opacity&#39;, &#39;name&#39;, &#39;uid&#39;, &#39;ids&#39;, &#39;customdata&#39;, &#39;selectedpoints&#39;, &#39;hoverinfo&#39;, &#39;hoverlabel&#39;, &#39;stream&#39;, &#39;transforms&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;text&#39;, &#39;delaunayaxis&#39;, &#39;alphahull&#39;, &#39;intensity&#39;, &#39;color&#39;, &#39;vertexcolor&#39;, &#39;facecolor&#39;, &#39;cauto&#39;, &#39;cmin&#39;, &#39;cmax&#39;, &#39;colorscale&#39;, &#39;autocolorscale&#39;, &#39;reversescale&#39;, &#39;showscale&#39;, &#39;colorbar&#39;, &#39;flatshading&#39;, &#39;contour&#39;, &#39;lightposition&#39;, &#39;lighting&#39;, &#39;xcalendar&#39;, &#39;ycalendar&#39;, &#39;zcalendar&#39;, &#39;scene&#39;, &#39;idssrc&#39;, &#39;customdatasrc&#39;, &#39;hoverinfosrc&#39;, &#39;xsrc&#39;, &#39;ysrc&#39;, &#39;zsrc&#39;, &#39;isrc&#39;, &#39;jsrc&#39;, &#39;ksrc&#39;, &#39;textsrc&#39;, &#39;intensitysrc&#39;, &#39;vertexcolorsrc&#39;, &#39;facecolorsrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; matrixB &lt;- matrix(c(1,2,3,2,2,1,3,2,4),3) matrixB ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 2 2 ## [3,] 3 1 4 drawMatrixWithDet(matrixB,dim(matrixB)[1]) ## Warning: &#39;mesh3d&#39; objects don&#39;t have these attributes: &#39;mode&#39;, &#39;line&#39; ## Valid attributes include: ## &#39;type&#39;, &#39;visible&#39;, &#39;showlegend&#39;, &#39;legendgroup&#39;, &#39;opacity&#39;, &#39;name&#39;, &#39;uid&#39;, &#39;ids&#39;, &#39;customdata&#39;, &#39;selectedpoints&#39;, &#39;hoverinfo&#39;, &#39;hoverlabel&#39;, &#39;stream&#39;, &#39;transforms&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;text&#39;, &#39;delaunayaxis&#39;, &#39;alphahull&#39;, &#39;intensity&#39;, &#39;color&#39;, &#39;vertexcolor&#39;, &#39;facecolor&#39;, &#39;cauto&#39;, &#39;cmin&#39;, &#39;cmax&#39;, &#39;colorscale&#39;, &#39;autocolorscale&#39;, &#39;reversescale&#39;, &#39;showscale&#39;, &#39;colorbar&#39;, &#39;flatshading&#39;, &#39;contour&#39;, &#39;lightposition&#39;, &#39;lighting&#39;, &#39;xcalendar&#39;, &#39;ycalendar&#39;, &#39;zcalendar&#39;, &#39;scene&#39;, &#39;idssrc&#39;, &#39;customdatasrc&#39;, &#39;hoverinfosrc&#39;, &#39;xsrc&#39;, &#39;ysrc&#39;, &#39;zsrc&#39;, &#39;isrc&#39;, &#39;jsrc&#39;, &#39;ksrc&#39;, &#39;textsrc&#39;, &#39;intensitysrc&#39;, &#39;vertexcolorsrc&#39;, &#39;facecolorsrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; 8.1.1 More that 3D You have to let it all go, Neo. Fear, doubt, and disbelief. Free your mind Morpheus Well, you have to stuck to the formulas… This will make sense once you can bind the visual insights to the formula insights to under 3D. 8.2 EigenValues and EigenVectors 8.2.1 Eigen Values \\[ |S-lI| = 0 \\] Subtracting \\(lI\\) from \\(S\\) will create a matrix with determinant equals zero, meaning all of its vectors lay on a single line - all the vectors have the same span 8.2.2 Eigen Vectores \\[ Sx = lx \\] I want an \\(x\\) vector that, when rotated by \\(S\\), the result is the same than scaling \\(x\\) by \\(l\\). 8.3 Spectral Decomposition "],
["exercises-1.html", "9 Exercises", " 9 Exercises "],
["exercise-1.html", "10 Exercise 10.1 Exercise 1 - Linear Algebra 10.2 10.3 11 10.4 12 10.5 13 10.6 14 10.7 15 10.8 16 10.9 17 10.10 18 10.11 19 10.12 24 10.13 25", " 10 Exercise 10.1 Exercise 1 - Linear Algebra 10.1.1 Ex 1 A = matrix(c(4,7,2,5,3,8),2) B = matrix(c(3,6,-2,9,4,-5),2) \\[ A + B = \\] A+B ## [,1] [,2] [,3] ## [1,] 7 0 7 ## [2,] 13 14 3 \\[ A - B = \\] A-B ## [,1] [,2] [,3] ## [1,] 1 4 -1 ## [2,] 1 -4 13 \\[ A&#39; \\times A = \\] t(A)%*%A ## [,1] [,2] [,3] ## [1,] 65 43 68 ## [2,] 43 29 46 ## [3,] 68 46 73 \\[ A \\times A&#39; = \\] A%*%t(A) ## [,1] [,2] ## [1,] 29 62 ## [2,] 62 138 10.1.2 Ex 2 A = matrix(c(1,2,3,-1),2) B = matrix(c(2,1,0,5),2) \\[ A \\times B = \\] A %*% B ## [,1] [,2] ## [1,] 5 15 ## [2,] 3 -5 \\[ B \\times A = \\] B %*% A ## [,1] [,2] ## [1,] 2 6 ## [2,] 11 -2 \\[ det(A \\times B) = \\] det(A %*% B ) ## [1] -70 \\[ det(A) = \\] det(A) ## [1] -7 \\[ det(B) = \\] det(B) ## [1] 10 10.1.3 Ex 3 A = matrix(c(1,2,5,2,4,10,3,6,15),3) B = matrix(c(-1,-1,1,1,1,-1,-2,-2,2),3) C = matrix(c(1,1,2,2,2,2,4,4,4),3) \\[ A \\times B = 0 \\] A %*% B ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 A %*% B[,1] ## [,1] ## [1,] 0 ## [2,] 0 ## [3,] 0 \\[ tr(A)\\] psych::tr(A) ## [1] 20 \\[ tr(B)\\] psych::tr(B) ## [1] 2 It is important to notice that the matrix A is a matrix in which all of its vectors are linear dependent - aka they are all in the same span: drawMatrixWithDet(A, dim(A)[1]) ## Warning: &#39;mesh3d&#39; objects don&#39;t have these attributes: &#39;mode&#39;, &#39;line&#39; ## Valid attributes include: ## &#39;type&#39;, &#39;visible&#39;, &#39;showlegend&#39;, &#39;legendgroup&#39;, &#39;opacity&#39;, &#39;name&#39;, &#39;uid&#39;, &#39;ids&#39;, &#39;customdata&#39;, &#39;selectedpoints&#39;, &#39;hoverinfo&#39;, &#39;hoverlabel&#39;, &#39;stream&#39;, &#39;transforms&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;text&#39;, &#39;delaunayaxis&#39;, &#39;alphahull&#39;, &#39;intensity&#39;, &#39;color&#39;, &#39;vertexcolor&#39;, &#39;facecolor&#39;, &#39;cauto&#39;, &#39;cmin&#39;, &#39;cmax&#39;, &#39;colorscale&#39;, &#39;autocolorscale&#39;, &#39;reversescale&#39;, &#39;showscale&#39;, &#39;colorbar&#39;, &#39;flatshading&#39;, &#39;contour&#39;, &#39;lightposition&#39;, &#39;lighting&#39;, &#39;xcalendar&#39;, &#39;ycalendar&#39;, &#39;zcalendar&#39;, &#39;scene&#39;, &#39;idssrc&#39;, &#39;customdatasrc&#39;, &#39;hoverinfosrc&#39;, &#39;xsrc&#39;, &#39;ysrc&#39;, &#39;zsrc&#39;, &#39;isrc&#39;, &#39;jsrc&#39;, &#39;ksrc&#39;, &#39;textsrc&#39;, &#39;intensitysrc&#39;, &#39;vertexcolorsrc&#39;, &#39;facecolorsrc&#39;, &#39;key&#39;, &#39;set&#39;, &#39;frame&#39;, &#39;transforms&#39;, &#39;_isNestedKey&#39;, &#39;_isSimpleKey&#39;, &#39;_isGraticule&#39;, &#39;_bbox&#39; No, the graph is not wrong. All of the vectors are in the same span. And since they are linear dependent, the determinant is 0. \\[ det(A)\\] det(A) ## [1] 0 10.2 A &lt;- matrix(c(3,1,1,1,0,2,1,2,0),3) eigenObj &lt;- eigen(A) Lambda &lt;- eigenObj$values*diag(3) P &lt;- eigenObj$vectors Pline &lt;- t(P) A.spectralDecomp &lt;- P%*%(Lambda)%*%Pline A.spectralDecomp.2 &lt;- matrix(rep(0,9),3) for(i in 1:nrow(Lambda)){ A.spectralDecomp.2 = A.spectralDecomp.2 + eigenObj$values[i]*P[,i]%*%t(P[,i]) } 10.3 11 dados11 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe11.xlsx&quot;), col_names=F)) plot(x=dados11$X__1,y=dados11$X__2) plot(x=dados11$X__1,y=dados11$X__3) plot(x=dados11$X__2,y=dados11$X__3) xbars &lt;- colMeans(dados11) S &lt;- var(dados11) CorrMatrix &lt;- cor(dados11) 10.4 12 dados12 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe12.xlsx&quot;), col_names=F)) plot(x=dados12$X__1,y=dados12$X__2) plot(x=dados12$X__5,y=dados12$X__3) plot(x=dados12$X__2,y=dados12$X__3) xbars &lt;- colMeans(dados12) S &lt;- var(dados12) CorrMatrix &lt;- cor(dados12) 10.5 13 dados13 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe13.xlsx&quot;), col_names=F)) xs &lt;- c(1,2,3) ys &lt;- c(4,5) xsbar &lt;-colMeans(dados13[,xs]) ysbar &lt;-colMeans(dados13[,ys]) S &lt;- var(dados13) S11 &lt;- S[xs,xs] S12 &lt;- S[xs,ys] S21 &lt;- S[ys,xs] S22 &lt;- S[ys,ys] 10.6 14 dados13 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe13.xlsx&quot;), col_names=F)) cLinear &lt;- c(3,-2,4,-1,1) xbar &lt;- colMeans(dados13) S &lt;- var(dados13) R &lt;- cor(dados13) zmeans &lt;- t(cLinear)%*%xbar zS &lt;- cLinear%*%S%*%cLinear #z = 3x1 ??? 2x2 + 4x3 ??? x4 + x5 . C &lt;- matrix(c(1,2,-1,1,-3,-2,1,1,1,1,-2,-2,1,-1,3),3) z_M_means &lt;- C%*%xbar Sz &lt;- C%*%var(dados13)%*%t(C) D &lt;- sqrt(diag(Sz))*diag(3) Dinv &lt;- solve(D) Ry &lt;- Dinv%*%Sz%*%Dinv 10.7 15 dados13 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe13.xlsx&quot;), col_names=F)) cLinear &lt;- c(3,-2,4,-1,1) xbar &lt;- colMeans(dados13) S &lt;- var(dados13) R &lt;- cor(dados13) S.eigen &lt;- eigen(S) S.det &lt;- prod(S.eigen$values) S.tr &lt;- sum(S.eigen$values) R.det &lt;- det(R) 10.8 16 dados16 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe16.xlsx&quot;), col_names=F)) dados16.xbar &lt;- colMeans(dados16) dados16.S &lt;- var(dados16) dados16.R &lt;- cor(dados16) dados16.S.det &lt;- det(dados16.S) dados16.S.tr &lt;- sum(diag(dados16.S)) #z=x1+2x2+x3???3x4 a &lt;- c(1,2,1,-3) z &lt;- t(a%*%t(dados16)) #w=???2x1+3x2???x3+2x4 b &lt;- c(-2,3,-1,2) w &lt;- t(b%*%t(dados16)) zbar &lt;- a%*%dados16.xbar wbar &lt;- b%*%dados16.xbar z.variance &lt;- a%*%dados16.S%*%a w.variance &lt;- b%*%dados16.S%*%b corr_between_z_w &lt;- (a%*%dados16.S%*%b)/(sqrt(a%*%dados16.S%*%a)*sqrt(b%*%dados16.S%*%b)) 10.9 17 M &lt;- matrix(c(-1,2,5,3,4,2,-2,2,3),3) Ones &lt;- matrix(c(1,1,1,1,1,1,1,1,1),3) colMeans(M) -&gt; M.colMeans M - Ones*M.colMeans ## [,1] [,2] [,3] ## [1,] -3 1 -4 ## [2,] -1 1 -1 ## [3,] 4 1 2 Smatrixes &lt;- 1/(dim(M)[1]-1)*t(M)%*%(diag(dim(M)[1]) - (1/dim(M)[1])*Ones)%*%M det(Smatrixes) ## [1] 2.173262e-14 prod(eigen(Smatrixes)$values) ## [1] 7.226065e-15 sum(eigen(Smatrixes)$values) ## [1] 17 10.10 18 TODO Question: How do I calculate the S_uv? dados18 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe18.xlsx&quot;), col_names=F)) dados18.S &lt;- var(dados18) UC &lt;- matrix(c(1,1,1,-2,1,2,0,0,0,0,0,0),2) VC &lt;- matrix(c(0,0,0,0,0,0,0,0,0,3,-1,2,1,-2,-3,-2,1,1),3) U.S &lt;- UC%*%dados18.S%*%t(UC) V.S &lt;- VC%*%dados18.S%*%t(VC) 10.11 19 dados19 &lt;- as.data.frame(readxl::read_xlsx(file.path(datasetsDir,&quot;..&quot;,&quot;Exercicios&quot;,&quot;exe19.xlsx&quot;))) dados19.xbar &lt;- colMeans(dados19) getStats &lt;- function(data){ statsAlturas &lt;- summary(data) return( list( mean = statsAlturas[4], var = var(data), sd = sd(data), min = statsAlturas[1], max = statsAlturas[6], quartis = c(statsAlturas[2],statsAlturas[3],statsAlturas[5]) ) ) } Alturas &lt;- getStats(c(dados19$AlturasA,dados19$AlturasB)) Pesos &lt;- getStats(c(dados19$PesosA,dados19$PesosB)) plts &lt;- sapply(colnames(dados19),function(col){ hist(dados19[,col],breaks=20, main=paste0(&quot;Histogram of &quot;,col)) }) plts &lt;- sapply(colnames(dados19),function(col){ boxplot(dados19[,col],breaks=20, main=paste0(&quot;Histogram of &quot;,col)) }) H0_populationalMean &lt;- 170 alpha &lt;- 0.05 testStats &lt;- function(populationMean, sampleMean, sampleVar, sampleSize){ (sampleMean-populationMean)/(sampleVar/sqrt(sampleSize)) } tObs &lt;- testStats(H0_populationalMean, mean(dados19$AlturasA), var(dados19$AlturasA), length(dados19$AlturasA)) abs(tObs) &gt; qt(1-alpha,length(dados19$AlturasA)-1) ## [1] FALSE p_value &lt;- (1 - pt(abs(tObs),length(dados19$AlturasA)-1))*2 p_value &lt; 1-pt(1-alpha/2,length(dados19$AlturasA)-1) ## [1] FALSE H0_populationMean.pesos &lt;- 70 tObs.pesosA &lt;- testStats(H0_populationMean.pesos, mean(dados19$PesosA), var(dados19$PesosA), length(dados19$PesosA)) abs(tObs.pesosA) &gt; qt(1-alpha,length(dados19$PesosA)-1) ## [1] FALSE p_value.pesosA &lt;- (1 - pt(abs(tObs.pesosA),length(dados19$PesosA)-1))*2 10.12 24 matrix20 &lt;- matrix(c(3,4,5,4,6,4,7,7),4) mewPop &lt;- colMeans(matrix20) SigmaPop &lt;- (dim(matrix20)[1]-1)*(var(matrix20)/dim(matrix20)[1]) 10.13 25 Chi Sqrt with 6 freedom degrees matrix20 &lt;- matrix(c(3,4,5,4,6,4,7,7),4) mewPop &lt;- colMeans(matrix20) SigmaPop &lt;- (dim(matrix20)[1]-1)*(var(matrix20)/dim(matrix20)[1]) 1+1 "]
]
